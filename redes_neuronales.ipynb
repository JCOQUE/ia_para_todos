{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "241e8a5a",
      "metadata": {
        "id": "241e8a5a"
      },
      "source": [
        "# Clase IV: Redes neuronales\n",
        "\n",
        "\n",
        "* La Inteligencia Artificial se ha basado en imitar la inteligencia de la misma manera que esta se produce.\n",
        "\n",
        "\n",
        "* El cerebro es el órgano encargado de \"gestionar la inteligencia\" y por tanto parece sensato imitar su comportamiento para aprender a resolver determinadas tareas.\n",
        "\n",
        "\n",
        "* Es muy poco lo que se conoce del cerebro, pero haciendo una gran abstracción podemos simplificar el cerebro como un conjunto de neuronas conectadas entre sí que se comunican mediante señales eléctricas y químicas. El cerebro humano está formado por:\n",
        "    + $10^{11}$ Neuronas\n",
        "    + $10^{14}$ Conexiones\n",
        "    + Cada Neurona esta conectada a otras 1.000 y 200.000 Neuronas\n",
        "    \n",
        "\n",
        "* El funcionamiento (a un alto nivel de abstracción) de una neurona lo podemos describir de la siguiente manera:\n",
        "    1. Una neurona recibe impulsos (tanto excitatorios como inhibidores) de diferente intensidad de muchas otras neuronas.\n",
        "    2. La neurona suma (integra) los impulsos recibidos en el espacio y el tiempo.\n",
        "    3. Si la señal integrada resultante está por encima de un umbral, la neurona se dispara; es decir, emite un impulso eléctrico de una determinada intensidad.\n",
        "    6. La señal se transmite a la siguiente neurona en la red a través de una sinapsis (conexiones sinápticas) mediante neurotransmisores.\n",
        "    \n",
        "\n",
        "## El Perceptrón\n",
        "\n",
        "\n",
        "* En 1957 el Psicólogo Frank Rosenblatt introduce el Perceptrón como un dispositivo hardware con capacidad de autoaprendizaje.\n",
        "\n",
        "* Este Perceptrón que pretende imitar la arquitectura de un cerebro humano, consta de una red con:\n",
        "    + Una capa de entrada con $N$ neuronas (i.e. $N$ variables de entrada -> variables independientes).\n",
        "    + Una capa de salida de 1 neurona.\n",
        "    + Interconexiones entre todas las neuronas de la capa de entrada con todas las neuronas de la capa de salida.\n",
        "    + Aplica una combinación lineal de todas estas interconexiones.\n",
        "    + La función de activación es de tipo signo:\n",
        "    \n",
        "  $$\n",
        "        f(x)=\\begin{cases} 0 & \\text{si } x < 0 \\\\\n",
        "        1 & \\text{si } x \\geq 0\n",
        "        \\end{cases}\n",
        "  $$\n",
        "    \n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_05_Perceptron.png?raw=1\" style=\"width: 400px;\"/>\n",
        "\n",
        "\n",
        "* Generalizando el comportamiento de una Neurona Artificial, describimos el Perceptrón como:\n",
        "    + '$N$' neuronas de entrada {$e_1$, ..., $e_n$}.\n",
        "    + Conexión a masa $b_0$, también conocido como bias.\n",
        "    + Pesos {$w_0$, $w_1$, ..., $w_n$} que interconectan las entradas con la neurona.\n",
        "    + Función de entrada que es la suma de todas las entradas por sus pesos: $f_{entrada}= b_0 w_0 + \\sum (e_i w_i)$\n",
        "    + Salida dada por una función de activación $Funcion\\ Activacion= F(f_{entrada})$\n",
        "    \n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_08_Perceptron.png?raw=1\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "### Funciones de Activación\n",
        "\n",
        "\n",
        "#### Para capas ocultas\n",
        "* Una función de activación es una función que transmite la información generada por la combinación lineal de los pesos y las entradas.\n",
        "\n",
        "\n",
        "* A continuación se muestran las funciones de activación más utilizadas:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_09_funciones_activacion.png?raw=1\" style=\"width: 950px;\"/>\n",
        "\n",
        "\n",
        "* Dependiendo la función de activación que usemos, podremos resolver un tipo de problemas u otros.\n",
        "\n",
        "### Para capa de salida:\n",
        "\n",
        "- **Lineal** para problemas de regresión con posibles valores negativos.\n",
        "- **ReLu** para porblemas de regresión con un dominio solo positivo.\n",
        "- **Sigmoide** para clasificaciones binarias.\n",
        "- **Softmax** para clasificaciones multi-clase.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_10_regresion.png?raw=1\" style=\"width: 400px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_11_clasificacion.png?raw=1\" style=\"width: 400px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "Por tanto, un perceptrón puede servir tanto para problemas de **regresión** como de **clasificación** (entre otras cosas porque también para imágenes, aunque hay arquitecturas más específicas para este tipo de problemas, pero está bien saber que también pueden hacer eso)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683a096c",
      "metadata": {
        "id": "683a096c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "# El Perceptrón Multicapa\n",
        "\n",
        "\n",
        "* Utilizar un Perceptrón con una sola neurona no nos permitiría resolver problemas de regresión y clasificación complejos. Para ello necesitamos combinar varios Perceptrones poniéndolos en paralelo (capas) y uniéndolas en serie (conectando capas).\n",
        "\n",
        "\n",
        "* La unión de estos Perceptrones en capas en serie dio nombre al \"Deep Learning\", viniendo la palabra \"Deep\" de la profundidad que estas capas.\n",
        "\n",
        "\n",
        "* Aunque el Perceptrón Multicapa ya se definió hace muchas décadas, su uso no se ha popularizado hasta la primera década del siglo XXI ya que hasta esa fecha no se habían definido \"optimizadores\" (algoritmos que calculan los pesos de la red neuronal) capaces de calcular (o ajustar) los millones de parámetros que pueden tener estas redes neuronales profundas capaces de resolver problemas complejos.\n",
        "\n",
        "\n",
        "* Por tanto podemos ver un ***Perceptrón Multicapa*** como una ***estructura*** con las siguientes características:\n",
        "    1. ***'N' Neuronas de Entrada***: 'N' será el número de variables de entrada que tenga nuestro problema a resolver.\n",
        "    2. ***'C' capas ocultas con 'K' neuronas por capa oculta***: El número de capas ocultas y el número de neuronas por capa, se establecerán en función de lo complejo que sea el problema a resolver. Cuantas más capas y neuronas tenga nuestra red más complejo será nuestro modelo, pudiendo acarrear problemas de overfitting y por el contrario si tiene pocas capas oculta y pocas neuronas por capa, nuestro modelo será poco complejo y podrá acarrear problemas de underfitting. Por tanto hay que definir una arquitectura de red adecuada a la complejidad del problema que queramos resolver, intentando siempre encontrar una arquitectura de red lo más sencilla posible que sea capaz de resolver nuestro problema.\n",
        "    3. ***'M' Neuronas de Salida***: El número de neuronas de salida dependerá del problema a resolver, siendo estos:\n",
        "        - ***Problema de regresión***: Debe de tener ***1 neurona de salida con función de activación lineal***.\n",
        "        - ***Problema de clasificación***:\n",
        "            + ***Binaria: 1 Neurona con función de activación sigmoidal o tangente hiperbolica***.\n",
        "            + ***Múltiple***: Tiene que tener ***tantas neuronas como clases a clasificar***. Se recomienda que la ***función de activación sea una softmax***.\n",
        "            \n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_12_preceptron_multicapa.png?raw=1\" style=\"width: 900px;\"/>\n",
        "\n",
        "\n",
        "## Overfitting\n",
        "\n",
        "##### Regularización L1 y Regularizacion L2\n",
        "- Penalizan pesos altos en las redes neuronales. Un peso alto conduce a un modelo a exagerar la importancia de una variable, pero más importante, a generar un modelo con una varianza alta y más complejo.\n",
        "\n",
        "\n",
        "##### Dropout\n",
        "\n",
        "\n",
        "* El Dropout es un método que se utiliza para la regularización, que tiene como objetivo reducir el overfitting.\n",
        "\n",
        "\n",
        "* Consiste en perturbar la red en cada pasada de entrenamiento (feed-forward y backpropagation), eliminando al azar algunas de las unidades de cada capa.\n",
        "\n",
        "\n",
        "* El objeto es que al introducir ruido en el proceso de entrenamiento evitamos el overfiting, pues en cada paso de la iteración estamos limitando el número de unidades que la red puede usar para ajustar las respuestas.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_16_dropout.png?raw=1\" style=\"width: 1000px;\"/>\n",
        "\n",
        "## Otros hiperparámetros\n",
        "\n",
        "##### Función de Coste / Función de Pérdida\n",
        "Función, como en cualquier otro algoritmo de *Machine Learning*, para decirle matemáticamente al modelo:\n",
        "- \"Muy mal, hay que mejorar\"\n",
        "- \"Te estás acercando, mejora un poco más\"\n",
        "\n",
        "##### Optimizador\n",
        "Función complementaria a la función de coste. La función de coste te dice cómo de bien ha predicho el modelo en la última iteración y, por ende, si hay que mejorar/corregir los pesos. El optimizador es la función que impone cómo mejorar/corregir los pesos.\n",
        "\n",
        "\n",
        "##### Learning Rate\n",
        "El learning rate, $/alpha$, indica en qué grado se modifican los pesos para mejorar el modelo:\n",
        "- Un learning alto implica que en cada iteración los pesos se van a modificar mucho.\n",
        "- Un learning rate bajo implica que en cada iteración los pesos se van a modificar poco.\n",
        "\n",
        "\n",
        "##### Epochs\n",
        "\n",
        "\n",
        "* Los Epochs (las épocas en Castellano) es un hiperparámetro que indica el número de veces que la Red Neuronal aprenderá de todas las observaciones de Dataset.\n",
        "\n",
        "\n",
        "* Por ejemplo, si tenemos un Dataset con 200 observaciones y le indicamos a la red que realice 50 epochs, esto significa que la Red Neuronal leerá y aprenderá 50 veces las 200 observaciones del Dataset; es decir, que leerá 10.000 observaciones (200 x 50).\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_14_epoch.png?raw=1\" style=\"width: 900px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "##### Batch Size\n",
        "\n",
        "\n",
        "* El Batch Size es un hiperparámetro que indica el número de observaciones que tiene que leer la Red Neuronal antes de actualizar el modelo (los pesos de la Red Neuronal).\n",
        "\n",
        "* Por ejemplo, con un Batch Size de 100 lo que haremos será calcular la salida para 100 Observaciones y calcular sus errores en función de la predicción que realice la Red. Posteriormente se calcula el error medio de las 100 observaciones y se actualizan los pesos de la Red Neuronal.\n",
        "\n",
        "* Consideraciones importantes:\n",
        "    + Batch Size pequeño:  la Red Neuronal aprenda muy bien pero tardará mucho tiempo en calcular el modelo.\n",
        "    + Barch Size grande: la Red Neuronal no aprenda tan bien pero tardará menos tiempo en calcular el modelo.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/JCOQUE/ia_para_todos/blob/main/imgs/6_01_01_15_batchsize.png?raw=1\" style=\"width: 900px;\"/>\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "Hiperparámetro que ayuda a mejorar la eficiencia de las redes neuronales. Consiste en normalizar los valores que se envían entre capa y capa de la red neuronal.\n",
        "\n",
        "\n",
        "\n",
        "## ¿Cómo entrenar una red neuronal (Perceptron Multicapa)?\n",
        "\n",
        "\n",
        "* Para entrenar una red neuronal es necesario seguir los siguientes pasos:\n",
        "\n",
        "    1. ***Recopilar conjunto de datos*** (Cuantos más datos mejor).\n",
        "    2. ***Diseñar una función de perdida*** (loss function) apropiada para el problema; por ejemplo:\n",
        "        + ***MSE*** para problemas de regresión.\n",
        "        + ***Cross Entropy*** para problemas de clasificación (Clasificación binaria: “binary crossentropy” y Clasificación Múltiple: “categorical_crossentropy”).\n",
        "    3. ***Definir la arquitectura de la Red Neuronal*** y sus hiperparámetros.\n",
        "        + Número de Capas y Neuronas por Capa.\n",
        "        + Funciones de Activación.\n",
        "        + Hiperparámetros: Learning Rate, Regularization Rate, Epochs, Batch Size, etc.\n",
        "    4. ***Aplicar un algoritmo de optimización*** para minimizar la función de pérdida para que ajuste los pesos de la red (SGD: Stochastic Gradient Descent, RMSProp, Adam, etc.)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0fabcf",
      "metadata": {
        "id": "eb0fabcf"
      },
      "source": [
        "### 1.- Ejemplo de Clasificación con MLP (MultiLayer Perceptron)\n",
        "\n",
        "\n",
        "* A continuación vamos a ver un ejemplo (paso por paso) de cómo resolver un problema de clasificación utilizando un perceptron multicapa.\n",
        "\n",
        "\n",
        "* Recordemos que para entrenar una red neuronal es necesario seguir los siguientes pasos:\n",
        "\n",
        "    1. ***Recopilar conjunto de datos***\n",
        "    2. ***Diseñar una función de perdida***: Dado que estamos resolviendo un problema de clasificación, se propone utilizar como función de perdida:\n",
        "        + binary crossentropy, para clasificación binaria\n",
        "        + categorical_crossentropy, para clasificación multiple\n",
        "    3. ***Definir la arquitectura de la Red Neuronal*** y sus hiperparámetros.\n",
        "        + Capa de Entrada: Tantas neuronas como variables tenga nuestro problema\n",
        "        + Numero de capas y neuronas por capa: Debemos de indicar una función de activación\n",
        "        + ¿Dropout?¿Batch Normalization?\n",
        "        + Capa de Salida: Al tratarse de un problema de clasificación, tendremos:\n",
        "            - 1 Neurona si se trata de clasificación binaria. Se propone una función de activación sigmoidal.\n",
        "            - 'N' Neuronas si se trata de una clasificación múltiple con 'N' clases. Se propone una función de activación softmax.\n",
        "        + Hiperparámetros: Learning Rate, Regularization Rate, Epochs, Batch Size, etc.\n",
        "    4. ***Aplicar un algoritmo de optimización*** para minimizar la función de pérdida para que ajuste los pesos de la red (SGD: Stochastic Gradient Descent, RMSProp, Adam, etc.)\n",
        "\n",
        "\n",
        "* Vamos a resolver el problema de predicción de Churn\n",
        "\n",
        "\n",
        "### 1.1.- Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b788bf",
      "metadata": {
        "id": "86b788bf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./sesion_IV/data/Churn.csv\")\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8a1108",
      "metadata": {
        "id": "af8a1108"
      },
      "source": [
        "## 1.2.- Transformamos las variables categóricas y normalizamos los datos\n",
        "\n",
        "* Cuando trabajamos con redes neuronales **es muy importante tener los datos normalizados** para facilitar el entrenamiento de la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdfe74e",
      "metadata": {
        "id": "8bdfe74e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
        "\n",
        "\n",
        "# Binarizamos la varible género\n",
        "lb = LabelBinarizer()\n",
        "df['Gender'] = lb.fit_transform(df['Gender'])\n",
        "\n",
        "\n",
        "# ONE-HOT encode de la variable Geography\n",
        "df_dummy = pd.get_dummies(df['Geography'])\n",
        "df = pd.concat([df, df_dummy], axis=1)\n",
        "df = df.drop(columns='Geography')\n",
        "\n",
        "\n",
        "# Ordenamos el DataFrame (features_cols + target_col)\n",
        "target_col = 'Exited'\n",
        "features_cols = df.loc[:, df.columns != target_col].columns.tolist()\n",
        "df = df[features_cols + [target_col]]\n",
        "\n",
        "\n",
        "# Normalizamos las variables - NO EL TARGET\n",
        "min_max_scaler = MinMaxScaler()\n",
        "df[features_cols] = min_max_scaler.fit_transform(df[features_cols])\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ae5adf",
      "metadata": {
        "id": "27ae5adf"
      },
      "source": [
        "## Partición datos en train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "463ba8ef",
      "metadata": {
        "id": "463ba8ef"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[features_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "# División de datos en entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3daa3b1",
      "metadata": {
        "id": "e3daa3b1"
      },
      "source": [
        "### 1.4.- Definimos la arquitectura de la red neuronal\n",
        "\n",
        "\n",
        "* Vamos a crear una red neuronal con la siguiente estructura:\n",
        "    + Capa de Entrada: vamos a necesitar 12 neuronas en la capa de entrada - tenemos 12 variables\n",
        "    + Capa oculta 1: 128 neuronas y función de activación 'relu'\n",
        "    + Capa oculta 2: 64 neuronas y función de activación 'relu'\n",
        "    + Capa oculta 3: 32 neuronas y función de activación 'relu'\n",
        "    + Capa de salida: 1 neurona con función de activación Sigmoidal - problema de clasificación binaria - salida: {0, 1}\n",
        "    \n",
        "    \n",
        "* Para definir la arquitectura de una red neuronal en TensorFlow lo hacemos con un objeto de la clase \"Sequential\" al que le vamos añadiendo capas \"Densas\" de neuronas.\n",
        "\n",
        "\n",
        "* Al final con la función \"summary()\" nos muestra una descripción de la red neuronal. En esta caso es una red neuronal con 12033 pesos (parámetros) a ajustar:\n",
        "    + Capa de entrada - Capa oculta 1 -> (12 neuronas) x (128 neuronas) + 128 bias = 1664 pesos\n",
        "    + Capa oculta 1 - Capa oculta 2 -> (128 neuronas) x (64 neuronas) + 64 bias = 8256 pesos\n",
        "    + Capa oculta 2 - Capa oculta 3 -> (64 neuronas) x (32 neuronas) + 32 bias = 2080 pesos\n",
        "    + Capa oculta 3 - Capa de salida -> (32 neuronas) x (1 neurona) + 1 bias = 33 pesos\n",
        "    + TOTAL = 1664 + 8256 + 2080 + 33 = 12033 pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ff7aa5",
      "metadata": {
        "id": "85ff7aa5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Definimos la Arquitectura de la Red Neuronal\n",
        "# Creamos el modelo\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2daeb575",
      "metadata": {
        "id": "2daeb575"
      },
      "source": [
        "### 1.5.- Diseño de la función de perdida y elección del optimizador - \"compilación del modelo\"\n",
        "\n",
        "\n",
        "* Con el método \"compile(*args)\" vamos a definir:\n",
        "    1. Función de perdida: Vamos a usar como función de perdida el binary_crossentropy al tratarse de un problema de clasificación binaria\n",
        "    2. Optimizados: En este caso utilizaremos el optimizado Adam (optimizer='adam')\n",
        "    \n",
        "    \n",
        "* Por otro lado también podemos definir que métricas de evaluación queremos monitorizar durante el entrenamiento de la red. En este caso particular vamos a monitorizar el Accuracy, la Precision y el Recall: Documentación a las métricas https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe8b4b6",
      "metadata": {
        "id": "abe8b4b6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import metrics\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy',\n",
        "                       metrics.Precision(),\n",
        "                       metrics.Recall()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c25ede",
      "metadata": {
        "id": "56c25ede"
      },
      "source": [
        "### 1.6- Entrenamiento de la red neuronal\n",
        "\n",
        "* Con el método \"fit(*args)\" entrenamos la red. A este método le vamos a pasar los siguientes parámetros:\n",
        "    + `X`: conjunto de datos de entrenamiento\n",
        "    + `y`: target del conjunto de datos de entrenamiento\n",
        "    + `epochs`: indicar cuantas veces leerá el dataset para entrenar\n",
        "    + `batch_size`: actuzalizará los pesos de la red tras evaluar el número de elementos del dataset que se le indique. Podemos indicar cuantos batches queremos tener en función del numero de elementos del dataset; por ejemplo, indicamos 150 batches diviendo el numero de elementos del dataset entre 150.\n",
        "    + `class_weight`: dado que se trata de un problema de clasificación binaria desbalanceado, podemos indicarle el peso que damos a las clases como un diccionario de pesos. En la documentación de TensorFlow indica cómo calcular el peso de las clases: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=es-419#class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a81718c",
      "metadata": {
        "id": "4a81718c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_class_0 = np.count_nonzero(y_train == 0)\n",
        "num_class_1 = np.count_nonzero(y_train == 1)\n",
        "num_total = len(y_train)\n",
        "weight_0 = (1 / num_class_0) * (num_total / 2.0)\n",
        "weight_1 = (1 / num_class_1) * (num_total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_0, 1: weight_1}\n",
        "\n",
        "print('Elementos Clase 0: {} - Peso de la clase 0: {:.2f}'.format(num_class_0, weight_0))\n",
        "print('Elementos Clase 1: {} - Peso de la clase 1: {:.2f}'.format(num_class_1, weight_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c57b04d",
      "metadata": {
        "id": "9c57b04d"
      },
      "outputs": [],
      "source": [
        "# Se fuerza a usar la CPU y no la GPU en caso de tener GPU en el ordenador\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aead5212",
      "metadata": {
        "id": "aead5212"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "history =model.fit(X_train,\n",
        "                   y_train,\n",
        "                   epochs=50,\n",
        "                   batch_size=int(X_train.shape[0]/150), # 8000 elementos / 150 batches -> batch_size 53 elementos\n",
        "                   class_weight=class_weight,\n",
        "                   verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096b36af",
      "metadata": {
        "id": "096b36af"
      },
      "source": [
        "### 1.7.- Evaluación del Modelo\n",
        "\n",
        "\n",
        "* Con la función \"predict()\" obtenemos las predicciones de los datos que les pasemos como parámetro. En este caso en el que estamos resolviendo un problema de clasificación, ***la predicción será el valor que nos devuelva la red neuronal que tiene como neurona de salida una función sigmoidal***, con lo cual no nos devolverá la etiqueta de la clase, si no que ***nos devolverá las probabilidad de pertenencia a la clase 1***. Para obtener la predicción de la clase debemos de:\n",
        "    + Asignar clase 0 a aquellas salidas con valor menor o igual a 0.5\n",
        "    + Asignar clase 1 a aquellas salidas con valor mayor a 0.5\n",
        "\n",
        "\n",
        "* Con la función \"evaluate()\" obtenemos los valores de las métricas que hemos ido monitorizando durante el proceso de entrenamiento\n",
        "\n",
        "\n",
        "* Con las predicciones podemos evaluar el rendimiento del modelo comparándolo con la salida esperada. Para ello podemos utilizar las funciones de evaluación proporcionadas por Scikit-Learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa1a510",
      "metadata": {
        "id": "ffa1a510"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las métricas con TensorFlow\n",
        "metrics_train = model.evaluate(X_train, y_train)\n",
        "metrics_test = model.evaluate(X_test, y_test)\n",
        "print(\"\\nNombres de las métricas: {}\".format(model.metrics_names))\n",
        "print(\"Resultados de las métricas Train: {}\".format([round(elem, 4) for elem in metrics_train]))\n",
        "print(\"Resultados de las métricas Test:  {}\\n\".format([round(elem, 4) for elem in metrics_test]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa0e425",
      "metadata": {
        "id": "4fa0e425"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Obtenemos las predicciones\n",
        "y_proba_predict_train = model.predict(X_train)\n",
        "y_proba_predict_test = model.predict(X_test)\n",
        "y_pred_train = np.where(y_proba_predict_train > 0.5, 1,0)\n",
        "y_pred_test = np.where(y_proba_predict_test > 0.5, 1,0)\n",
        "\n",
        "print(\"\\nAccuracy train: {:.4f}\".format(accuracy_score(y_true=y_train, y_pred=y_pred_train)))\n",
        "print(\"Accuracy test: {:.4f}\".format(accuracy_score(y_true=y_test, y_pred=y_pred_test)))\n",
        "print(\"Precision train: {:.4f}\".format(precision_score(y_true=y_train, y_pred=y_pred_train)))\n",
        "print(\"Precision test: {:.4f}\".format(precision_score(y_true=y_test, y_pred=y_pred_test)))\n",
        "print(\"Recall train: {:.4f}\".format(recall_score(y_true=y_train, y_pred=y_pred_train)))\n",
        "print(\"Recall test: {:.4f}\".format(recall_score(y_true=y_test, y_pred=y_pred_test)))\n",
        "print(\"F1 train: {:.4f}\".format(f1_score(y_true=y_train, y_pred=y_pred_train)))\n",
        "print(\"F1 test: {:.4f}\".format(f1_score(y_true=y_test, y_pred=y_pred_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65484fd4",
      "metadata": {
        "id": "65484fd4"
      },
      "source": [
        "#### Matrices de confusión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c4040c",
      "metadata": {
        "id": "61c4040c"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Definimos el heatmap de la matriz de confusión\n",
        "def plot_confusion_matrix(cm, classes, title, cmap=plt.cm.Greens):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, '{:.1f} %'.format(cm[i, j]), horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "# Obtenemos las matrices de confusión\n",
        "cfm_train = confusion_matrix(y_true=y_train, y_pred=y_pred_train)\n",
        "cfm_train = (cfm_train.astype('float') / cfm_train.sum(axis=1)[:, np.newaxis]) * 100\n",
        "cfm_test = confusion_matrix(y_true=y_test, y_pred=y_pred_test)\n",
        "cfm_test = (cfm_test.astype('float') / cfm_test.sum(axis=1)[:, np.newaxis]) * 100\n",
        "\n",
        "# Pintamos las matrices de confusión\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_confusion_matrix(cfm_train, classes=['0', '1'], title='Matriz de Confusión Datos Entrenamiento')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_confusion_matrix(cfm_test, classes=['0', '1'], title='Matriz de Confusión Datos Test')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff47bfe1",
      "metadata": {
        "id": "ff47bfe1"
      },
      "source": [
        "##### Reglas no escritas\n",
        "- El número de neuronas entre una capa oculta y la siguiente debe ser decreciente.\n",
        "- El número de neuronas en  las capas ocultas es un número potencia de 2 (2, 4, 8, 16, etc.)\n",
        "- La función de activación en las capas ocultas es ReLu (función que no satura la derivada y por tanto evita problemas de desvanecimiento de gradiente, pro puede provocar problemas con la explosión de gradiente).\n",
        "- El batch size es una potencia de 2, y entre 32 y 512.\n",
        "- Se normalizan los datos de entrada y se utiliza Batch Normalization siempre.\n",
        "\n",
        "\n",
        "\n",
        "##### Links adicionales\n",
        "[serie redes neuronales de DotCSV](https://www.youtube.com/watch?v=MRIv2IwFTPg&list=PL-Ogd76BhmcB9OjPucsnc2-piEE96jJDQ)\n",
        "\n",
        "[Descenso del Gradiente](https://www.youtube.com/watch?v=A6FiCDoz8_4&pp=ygUdZGVzY2Vuc28gZGVsIGdyYWRpZW50ZSBkb3Rjc3Y%3D)\n",
        "\n",
        "[serie redes neuronales de 3Blue1Brown (en  inglés)](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
        "\n",
        "[Playground redes neuronales](https://playground.tensorflow.org/)\n",
        "\n",
        "[Open AI Microscope](https://openai.com/index/microscope/)\n",
        "\n",
        "[TFG Javi Coque](https://github.com/JCOQUE/TFG-ingenieria/blob/main/Memoria/memoria-documento.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60960fe0",
      "metadata": {
        "id": "60960fe0"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}